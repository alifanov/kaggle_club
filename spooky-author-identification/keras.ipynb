{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Dense, GlobalAveragePooling1D, Embedding\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/train.csv')\n",
    "a2c = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\n",
    "y = np.array([a2c[a] for a in df.author])\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c MWS   EAP   HPL   \n",
      "y 14877 17001 12534 \n",
      "æ 0 36 10 \n",
      "X 4 17 5 \n",
      "s 45962 53841 43915 \n",
      "Å 0 0 1 \n",
      "q 677 1030 779 \n",
      ", 12045 17594 8581 \n",
      "T 1230 2217 1583 \n",
      "ñ 0 0 7 \n",
      "' 476 1334 1710 \n",
      "o 53386 67145 50996 \n",
      "x 1267 1951 1061 \n",
      "ë 0 0 12 \n",
      "ἶ 0 0 2 \n",
      "à 0 10 0 \n",
      "c 17911 24127 18338 \n",
      "ç 0 1 0 \n",
      "r 44042 51221 40590 \n",
      "f 18351 22354 16272 \n",
      "a 55274 68525 56815 \n",
      "t 63142 82426 62235 \n",
      "d 35315 36862 33366 \n",
      "N 204 411 345 \n",
      "W 681 739 732 \n",
      "n 50291 62636 50879 \n",
      "D 227 491 334 \n",
      "j 682 683 424 \n",
      "B 395 835 533 \n",
      "U 46 166 94 \n",
      "Ο 0 0 3 \n",
      "C 308 395 439 \n",
      "v 7948 9624 6529 \n",
      "A 943 1258 1167 \n",
      "α 0 0 2 \n",
      "G 246 313 318 \n",
      "Æ 0 1 4 \n",
      "z 400 634 529 \n",
      "S 578 729 841 \n",
      "ô 0 8 0 \n",
      "L 307 458 249 \n",
      "â 0 6 0 \n",
      ": 339 176 47 \n",
      "δ 0 0 2 \n",
      "ö 0 16 3 \n",
      "b 9611 13245 10636 \n",
      "P 365 442 320 \n",
      "ï 0 0 7 \n",
      "g 12601 16088 14951 \n",
      "Ν 0 0 1 \n",
      "é 0 47 15 \n",
      "î 0 1 0 \n",
      "O 282 414 503 \n",
      "i 46080 60952 44250 \n",
      "h 43738 51580 42770 \n",
      "M 415 1065 645 \n",
      "Σ 0 0 1 \n",
      "I 4917 4846 3480 \n",
      "; 2662 1354 1143 \n",
      "m 20471 22792 17622 \n",
      "Z 2 23 51 \n",
      "V 57 156 67 \n",
      "R 385 258 237 \n",
      "Υ 0 0 1 \n",
      "\" 1469 2987 513 \n",
      "e 97515 114885 88259 \n",
      "E 445 435 281 \n",
      "ü 0 1 5 \n",
      ". 5761 8406 5908 \n",
      "K 35 86 176 \n",
      "J 66 164 210 \n",
      "p 12361 17422 10965 \n",
      "F 232 383 269 \n",
      "w 16062 17507 15554 \n",
      "H 669 864 741 \n",
      "Q 7 21 10 \n",
      "? 419 510 169 \n",
      "ê 0 28 2 \n",
      "Y 234 282 111 \n",
      "u 21025 26311 19519 \n",
      "ä 0 1 6 \n",
      "l 27819 35371 30273 \n",
      "k 3707 4277 5204 \n",
      "Π 0 0 1 \n",
      "è 0 15 0 \n"
     ]
    }
   ],
   "source": [
    "counter = {name : defaultdict(int) for name in set(df.author)}\n",
    "for (text, author) in zip(df.text, df.author):\n",
    "    text = text.replace(' ', '')\n",
    "    for c in text:\n",
    "        counter[author][c] += 1\n",
    "\n",
    "chars = set()\n",
    "for v in counter.values():\n",
    "    chars |= v.keys()\n",
    "    \n",
    "names = [author for author in counter.keys()]\n",
    "\n",
    "print('c ', end='')\n",
    "for n in names:\n",
    "    print(n, end='   ')\n",
    "print()\n",
    "for c in chars:    \n",
    "    print(c, end=' ')\n",
    "    for n in names:\n",
    "        print(counter[n][c], end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.replace(\"' \", \" ' \")\n",
    "    signs = set(',.:;\"?!')\n",
    "    prods = set(text) & signs\n",
    "    if not prods:\n",
    "        return text\n",
    "\n",
    "    for sign in prods:\n",
    "        text = text.replace(sign, ' {} '.format(sign) )\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_docs(df, n_gram_max=2):\n",
    "    def add_ngram(q, n_gram_max):\n",
    "            ngrams = []\n",
    "            for n in range(2, n_gram_max+1):\n",
    "                for w_index in range(len(q)-n+1):\n",
    "                    ngrams.append('--'.join(q[w_index:w_index+n]))\n",
    "            return q + ngrams\n",
    "        \n",
    "    docs = []\n",
    "    for doc in df.text:\n",
    "        doc = preprocess(doc).split()\n",
    "        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 2\n",
    "\n",
    "docs = create_docs(df)\n",
    "tokenizer = Tokenizer(lower=False, filters='')\n",
    "tokenizer.fit_on_texts(docs)\n",
    "num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words, lower=False, filters='')\n",
    "tokenizer.fit_on_texts(docs)\n",
    "docs = tokenizer.texts_to_sequences(docs)\n",
    "\n",
    "maxlen = 256\n",
    "\n",
    "docs = pad_sequences(sequences=docs, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = np.max(docs) + 1\n",
    "embedding_dims = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(embedding_dims=20, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/25\n",
      "15663/15663 [==============================] - 13s 810us/step - loss: 1.0675 - acc: 0.4076 - val_loss: 1.0298 - val_acc: 0.4545\n",
      "Epoch 2/25\n",
      "15663/15663 [==============================] - 12s 785us/step - loss: 0.9330 - acc: 0.6121 - val_loss: 0.8592 - val_acc: 0.7160\n",
      "Epoch 3/25\n",
      "15663/15663 [==============================] - 13s 827us/step - loss: 0.7251 - acc: 0.7874 - val_loss: 0.7073 - val_acc: 0.7574\n",
      "Epoch 4/25\n",
      "15663/15663 [==============================] - 13s 829us/step - loss: 0.5658 - acc: 0.8475 - val_loss: 0.6097 - val_acc: 0.7870\n",
      "Epoch 5/25\n",
      "15663/15663 [==============================] - 12s 747us/step - loss: 0.4524 - acc: 0.8800 - val_loss: 0.5403 - val_acc: 0.8041\n",
      "Epoch 6/25\n",
      "15663/15663 [==============================] - 12s 758us/step - loss: 0.3664 - acc: 0.9067 - val_loss: 0.4878 - val_acc: 0.8195\n",
      "Epoch 7/25\n",
      "15663/15663 [==============================] - 12s 759us/step - loss: 0.2989 - acc: 0.9282 - val_loss: 0.4507 - val_acc: 0.8299\n",
      "Epoch 8/25\n",
      "15663/15663 [==============================] - 12s 754us/step - loss: 0.2450 - acc: 0.9446 - val_loss: 0.4190 - val_acc: 0.8450\n",
      "Epoch 9/25\n",
      "15663/15663 [==============================] - 12s 743us/step - loss: 0.2011 - acc: 0.9580 - val_loss: 0.3980 - val_acc: 0.8463\n",
      "Epoch 10/25\n",
      "15663/15663 [==============================] - 12s 773us/step - loss: 0.1655 - acc: 0.9666 - val_loss: 0.3852 - val_acc: 0.8488\n",
      "Epoch 11/25\n",
      "15663/15663 [==============================] - 12s 776us/step - loss: 0.1358 - acc: 0.9743 - val_loss: 0.3658 - val_acc: 0.8593\n",
      "Epoch 12/25\n",
      "15663/15663 [==============================] - 12s 747us/step - loss: 0.1119 - acc: 0.9803 - val_loss: 0.3589 - val_acc: 0.8634\n",
      "Epoch 13/25\n",
      "15663/15663 [==============================] - 12s 762us/step - loss: 0.0921 - acc: 0.9850 - val_loss: 0.3478 - val_acc: 0.8687\n",
      "Epoch 14/25\n",
      "15663/15663 [==============================] - 12s 765us/step - loss: 0.0763 - acc: 0.9876 - val_loss: 0.3429 - val_acc: 0.8685\n",
      "Epoch 15/25\n",
      "15663/15663 [==============================] - 12s 796us/step - loss: 0.0629 - acc: 0.9906 - val_loss: 0.3409 - val_acc: 0.8685\n",
      "Epoch 16/25\n",
      "15663/15663 [==============================] - 13s 800us/step - loss: 0.0522 - acc: 0.9923 - val_loss: 0.3413 - val_acc: 0.8700\n",
      "Epoch 17/25\n",
      "15663/15663 [==============================] - 13s 827us/step - loss: 0.0434 - acc: 0.9937 - val_loss: 0.3459 - val_acc: 0.8667\n"
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "x_train, x_test, y_train, y_test = train_test_split(docs, y, test_size=0.2)\n",
    "\n",
    "model = create_model()\n",
    "hist = model.fit(x_train, y_train,\n",
    "                 batch_size=16,\n",
    "                 validation_data=(x_test, y_test),\n",
    "                 epochs=epochs,\n",
    "                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = create_docs(df)\n",
    "tokenizer = Tokenizer(lower=True, filters='')\n",
    "tokenizer.fit_on_texts(docs)\n",
    "num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words, lower=True, filters='')\n",
    "tokenizer.fit_on_texts(docs)\n",
    "docs = tokenizer.texts_to_sequences(docs)\n",
    "\n",
    "maxlen = 256\n",
    "\n",
    "docs = pad_sequences(sequences=docs, maxlen=maxlen)\n",
    "\n",
    "input_dim = np.max(docs) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/25\n",
      "15663/15663 [==============================] - 13s 802us/step - loss: 1.0664 - acc: 0.4097 - val_loss: 1.0256 - val_acc: 0.4221\n",
      "Epoch 2/25\n",
      "15663/15663 [==============================] - 12s 756us/step - loss: 0.9239 - acc: 0.6188 - val_loss: 0.8481 - val_acc: 0.6816\n",
      "Epoch 3/25\n",
      "15663/15663 [==============================] - 13s 819us/step - loss: 0.7198 - acc: 0.7865 - val_loss: 0.6996 - val_acc: 0.7640\n",
      "Epoch 4/25\n",
      "15663/15663 [==============================] - 13s 808us/step - loss: 0.5658 - acc: 0.8396 - val_loss: 0.5995 - val_acc: 0.7962\n",
      "Epoch 5/25\n",
      "15663/15663 [==============================] - 13s 802us/step - loss: 0.4555 - acc: 0.8772 - val_loss: 0.5331 - val_acc: 0.8105\n",
      "Epoch 6/25\n",
      "15663/15663 [==============================] - 12s 774us/step - loss: 0.3719 - acc: 0.9023 - val_loss: 0.4810 - val_acc: 0.8322\n",
      "Epoch 7/25\n",
      "15663/15663 [==============================] - 12s 778us/step - loss: 0.3056 - acc: 0.9240 - val_loss: 0.4435 - val_acc: 0.8412\n",
      "Epoch 8/25\n",
      "15663/15663 [==============================] - 12s 784us/step - loss: 0.2525 - acc: 0.9387 - val_loss: 0.4138 - val_acc: 0.8470\n",
      "Epoch 9/25\n",
      "15663/15663 [==============================] - 12s 736us/step - loss: 0.2085 - acc: 0.9539 - val_loss: 0.3931 - val_acc: 0.8493\n",
      "Epoch 10/25\n",
      "15663/15663 [==============================] - 12s 758us/step - loss: 0.1729 - acc: 0.9636 - val_loss: 0.3734 - val_acc: 0.8555\n",
      "Epoch 11/25\n",
      "15663/15663 [==============================] - 11s 724us/step - loss: 0.1436 - acc: 0.9703 - val_loss: 0.3623 - val_acc: 0.8578\n",
      "Epoch 12/25\n",
      "15663/15663 [==============================] - 12s 743us/step - loss: 0.1195 - acc: 0.9770 - val_loss: 0.3494 - val_acc: 0.8621\n",
      "Epoch 13/25\n",
      "15663/15663 [==============================] - 12s 780us/step - loss: 0.0989 - acc: 0.9810 - val_loss: 0.3427 - val_acc: 0.8641\n",
      "Epoch 14/25\n",
      "15663/15663 [==============================] - 11s 721us/step - loss: 0.0830 - acc: 0.9855 - val_loss: 0.3414 - val_acc: 0.8639\n",
      "Epoch 15/25\n",
      "15663/15663 [==============================] - 11s 724us/step - loss: 0.0691 - acc: 0.9880 - val_loss: 0.3339 - val_acc: 0.8685\n",
      "Epoch 16/25\n",
      "15663/15663 [==============================] - 12s 748us/step - loss: 0.0580 - acc: 0.9913 - val_loss: 0.3411 - val_acc: 0.8644\n",
      "Epoch 17/25\n",
      "15663/15663 [==============================] - 12s 734us/step - loss: 0.0489 - acc: 0.9922 - val_loss: 0.3359 - val_acc: 0.8708\n"
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "x_train, x_test, y_train, y_test = train_test_split(docs, y, test_size=0.2)\n",
    "\n",
    "model = create_model()\n",
    "hist = model.fit(x_train, y_train,\n",
    "                 batch_size=16,\n",
    "                 validation_data=(x_test, y_test),\n",
    "                 epochs=epochs,\n",
    "                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./data/test.csv')\n",
    "docs = create_docs(test_df)\n",
    "docs = tokenizer.texts_to_sequences(docs)\n",
    "docs = pad_sequences(sequences=docs, maxlen=maxlen)\n",
    "y = model.predict_proba(docs)\n",
    "\n",
    "result = pd.read_csv('./data/sample_submission.csv')\n",
    "for a, i in a2c.items():\n",
    "    result[a] = y[:, i]\n",
    "result.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"id\",\"EAP\",\"HPL\",\"MWS\"\r\n",
      "\"id02310\",0.403493538995863,0.287808366106543,0.308698094897594\r\n",
      "\"id24541\",0.403493538995863,0.287808366106543,0.308698094897594\r\n",
      "\"id00134\",0.403493538995863,0.287808366106543,0.308698094897594\r\n",
      "\"id27757\",0.403493538995863,0.287808366106543,0.308698094897594\r\n",
      "\"id04081\",0.403493538995863,0.287808366106543,0.308698094897594\r\n",
      "\"id27337\",0.403493538995863,0.287808366106543,0.308698094897594\r\n",
      "\"id24265\",0.403493538995863,0.287808366106543,0.308698094897594\r\n",
      "\"id25917\",0.403493538995863,0.287808366106543,0.308698094897594\r\n",
      "\"id04951\",0.403493538995863,0.287808366106543,0.308698094897594\r\n"
     ]
    }
   ],
   "source": [
    "!head ./data/sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,EAP,HPL,MWS\r\n",
      "id02310,0.022793320938944817,0.009503418579697609,0.9677032232284546\r\n",
      "id24541,0.9998486042022705,0.00015134006389416754,5.5520096964301047e-08\r\n",
      "id00134,0.001943304785527289,0.9928625226020813,0.005194144323468208\r\n",
      "id27757,0.969866156578064,0.02821045182645321,0.0019233745988458395\r\n",
      "id04081,0.7887112498283386,0.07711251080036163,0.1341763436794281\r\n",
      "id27337,0.9975106716156006,0.0011480755638331175,0.0013412671396508813\r\n",
      "id24265,0.9875584840774536,0.009479498490691185,0.002961952006444335\r\n",
      "id25917,0.0029075471684336662,0.03873061388731003,0.958361804485321\r\n",
      "id04951,0.9998929500579834,8.331518620252609e-05,2.3681956008658744e-05\r\n"
     ]
    }
   ],
   "source": [
    "!head submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
